# Mini Project: **Interactive Educational Assistant**

### Objective
Create a Python script that sets up an interactive assistant for answering questions based on curated educational documents. The assistant will:
- Store and retrieve contextually relevant information.
- Generate answers based on retrieved content.
- Maintain a session history for an enriched, coherent conversation.

---

## Project Steps

### Step 1: Set Up the Vector Store

- **Function Name**: `initialize_vector_store`
- **Purpose**: This function will set up an in-memory vector store using educational documents.
- **Process**:
  - Create a list of educational content or sample text data (e.g., "Photosynthesis is...").
  - Embed each document using an embedding model (e.g., `AI21Embeddings`).
  - Store the embedded documents in an `InMemoryVectorStore`.
- **Output**: Returns a configured vector store ready for similarity-based search.

---

### Step 2: Define a Function for Document Search in Vector Store

- **Function Name**: `search_vector_store`
- **Purpose**: Retrieves relevant documents from the vector store based on a query, using cosine similarity.
- **Process**:
  - Take a user’s query and define the number of top results to return (e.g., `top_k=1`).
  - Perform a similarity search in the vector store to find the closest matching document(s).
  - Retrieve the most relevant document or section that will serve as context for the response.
- **Output**: Returns the top document based on similarity, which will be used as context in the response.

---

### Step 3: Create a Chat History Function

- **Function Name**: `initialize_chat_history`
- **Purpose**: Initializes an in-memory chat history to store user and assistant messages, enabling a coherent conversation.
- **Process**:
  - Set up an instance of `InMemoryChatMessageHistory`.
  - This will allow the assistant to keep track of the session context, ensuring that responses consider prior exchanges.
- **Output**: Returns a chat history instance that you can use to store and retrieve messages as the conversation progresses.

---

### Step 4: Define a Function to Generate Prompt Messages

- **Function Name**: `generate_prompt`
- **Purpose**: Constructs prompt messages that will guide the assistant’s response.
- **Process**:
  - Use a system message to define the assistant's role (e.g., “You are an educational assistant.”).
  - Include the retrieved document (from `search_vector_store`) as context in another system message.
  - Add the user’s query as a `HumanMessage` to finalize the prompt structure.
- **Output**: Returns a list of messages (system and human) that define the prompt for the assistant.

---

### Step 5: Create the Model Invocation Function for Generating Responses

- **Function Name**: `get_response_from_model`
- **Purpose**: Uses the prompt messages to generate a response from the language model (e.g., `ChatGroq`).
- **Process**:
  - Pass the generated prompt (from `generate_prompt`) to the language model’s `invoke` method.
  - Retrieve the model’s response, which will be based on the user’s query and the provided context.
- **Output**: Returns the content of the response generated by the language model.

---

### Step 6: Implement the Main Interaction Loop

- **Function Name**: `main`
- **Purpose**: Integrates all components into a single function to create an interactive educational assistant session.
- **Process**:
  - Initialize the vector store, embedding model, and chat history using the functions created in steps 1, 2, and 3.
  - Start a loop where you:
    - Accept a user query.
    - If the query is ‘exit,’ break the loop to end the session.
    - Search for relevant documents in the vector store using `search_vector_store`.
    - Generate a prompt using `generate_prompt`.
    - Get the model’s response with `get_response_from_model`.
    - Add each query and response to chat history, ensuring session continuity.
    - Print the assistant’s response to the user.
- **Output**: An interactive chat session where the assistant provides answers based on retrieved documents.

---

### Final Steps: Running the Assistant

1. At the end of the `.py` file, ensure you call the `main()` function to initiate the assistant when you run the script.
2. You now have a single Python file that:
   - Embeds documents into a vector store.
   - Uses similarity search to retrieve context.
   - Constructs prompts dynamically.
   - Generates responses using a language model.
   - Retains chat history for an enriched conversation.

This setup provides a unique, interactive educational assistant that uses RAG techniques, contextual retrieval, and conversational history in a single, self-contained file.
